import sys  # This is to read the cmd line args
import re
import json

if  len(sys.argv) < 2 :
    print("Lexer Error: Must provide source file in command line arguments.")
    exit(1)    

if not sys.argv[1].endswith('.c') :
    print("Lexer Error: Input must be a .c file for compilation. Compilation aborted.")
    exit(1)

# Open the source file we passed as our argument, after checking to make sure the file is a .c file!
source_file = open(sys.argv[1])
source_code = source_file.read()

# Remove all comments in source file (generated by GPT)
# Preprocessing step to remove comments
clean_code = re.sub(r'//.*?$|/\*.*?\*/',
                    lambda m: ''.join(c if c == '\n' else ' ' for c in m.group()),
                    source_code,
                    flags=re.DOTALL | re.MULTILINE)

# Using the following table for our tokens and their regular expressions
identifier = re.compile(r"[a-zA-Z_]\w*\b")
constant = re.compile(r"[0-9]+\b") 
keyword = re.compile(r"\b(int|void|return)\b")
open_parenth = re.compile(r"\(")
close_parenth = re.compile(r"\)")
open_brace = re.compile(r"{")
close_brace = re.compile(r"}")
semi_col = re.compile(r";")

found_tokens = []

def checkToken(token, lineNum) :
    if identifier.fullmatch(token) :
        if keyword.fullmatch(token) :
            print("kw:", token, "on line:", lineNum)
            found_tokens.append({"type": "kw", "value": token, "line": lineNum})
        else :
            print("id:", token, "on line:", lineNum)
            found_tokens.append({"type": "id", "value": token, "line": lineNum})
    elif constant.fullmatch(token) :
        print("const:", token, "on line:", lineNum)
        found_tokens.append({"type": "const","value": int(token),"line": lineNum})
    elif open_parenth.fullmatch(token) :
        print("opar:", token, "on line:", lineNum)
        found_tokens.append({"type": "opar", "value": token, "line": lineNum})
    elif close_parenth.fullmatch(token) :
        print("cpar:", token, "on line:", lineNum)
        found_tokens.append({"type": "cpar", "value": token, "line": lineNum})
    elif open_brace.fullmatch(token) :
        print("obra:", token, "on line:", lineNum)
        found_tokens.append({"type": "obra", "value": token, "line": lineNum})
    elif close_brace.fullmatch(token) :
        print("cbra:", token, "on line:", lineNum)
        found_tokens.append({"type": "cbra", "value": token, "line": lineNum})
    elif semi_col.fullmatch(token) :
        print("semcol:", token, "on line:", lineNum)
        found_tokens.append({"type": "semcol", "value": token, "line": lineNum})
    else :
        # Later I can change this to accept invalid tokens and let the parser deal with it
        sys.stderr.write(f"{"\033[91m"}{"Lexer error on invalid token\ninv: '"}{token}{"' on line: "}{lineNum}{"\033[0m"}\n")
        #exit(1)

test_string = ""

delimiters = ['{', '}', '(', ')', ';']

# Keeping track of line number
line_num = 1

for char in clean_code :
    if char == '\n' :
        line_num += 1
    # First check if we've reached whitespace
    if char.isspace() :
        # We've reached whitespace, this absolutely signals the end of a token
        if test_string :
            # Do comparisons
            checkToken(test_string, line_num)
            test_string = ""
        continue
    # Now check if we've reached a delimiter listed above
    if char in delimiters :
        if test_string :
            checkToken(test_string, line_num)
            test_string = ""
        checkToken(char, line_num)
        continue

    test_string += char

# If there's a token at the end with no delimiter after, check for it
# This would mean a compilation error because the syntax is wrong anyways
if test_string :
    checkToken(test_string, line_num)

# Close the source file
source_file.close()

# Shows the found tokens for the parser
print(found_tokens)

# Dump the tokens into a json file
with open("tokens.json", "w") as tokenFile :
    json.dump(found_tokens, tokenFile)

tokenFile.close()


